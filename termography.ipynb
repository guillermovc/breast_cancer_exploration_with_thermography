{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Breast Cancer detection using CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_FORCE_GPU_ALLOW_GROWTH'] = 'true'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\TE502801\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\TE502801\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\TE502801\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\TE502801\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\TE502801\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\TE502801\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "C:\\Users\\TE502801\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\TE502801\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\TE502801\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\TE502801\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\TE502801\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\TE502801\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorflow version: 1.14.0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from numpy import expand_dims\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import PIL\n",
    "import cv2\n",
    "import os\n",
    "from random import sample\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "from tensorflow.keras.preprocessing import image\n",
    "\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.preprocessing.image import img_to_array\n",
    "from tensorflow.keras.preprocessing.image import load_img\n",
    "\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.models import model_from_json, model_from_yaml\n",
    "\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Conv2D , MaxPool2D , Flatten , Dropout , BatchNormalization\n",
    "\n",
    "print(\"Tensorflow version:\", tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(tf.test.is_built_with_cuda())\n",
    "# print(tf.test.is_gpu_available(cuda_only=False,min_cuda_compute_capability=None))\n",
    "# print(tf.config.list_physical_devices('GPU'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function to save as pb "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def freeze_session(session, keep_var_names=None, output_names=None, clear_devices=True):\n",
    "    \"\"\"\n",
    "    Freezes the state of a session into a pruned computation graph.\n",
    "\n",
    "    Creates a new computation graph where variable nodes are replaced by\n",
    "    constants taking their current value in the session. The new graph will be\n",
    "    pruned so subgraphs that are not necessary to compute the requested\n",
    "    outputs are removed.\n",
    "\n",
    "    @param session The TensorFlow session to be frozen.\n",
    "    @param keep_var_names A list of variable names that should not be frozen,\n",
    "                          or None to freeze all the variables in the graph.\n",
    "    @param output_names Names of the relevant graph outputs.\n",
    "    @param clear_devices Remove the device directives from the graph for better portability.\n",
    "    @return The frozen graph definition.\n",
    "    \"\"\"\n",
    "    graph = session.graph\n",
    "    with graph.as_default():\n",
    "        freeze_var_names = list(set(v.op.name for v in tf.global_variables()).difference(keep_var_names or []))\n",
    "        output_names = output_names or []\n",
    "        output_names += [v.op.name for v in tf.global_variables()]\n",
    "        input_graph_def = graph.as_graph_def()\n",
    "        if clear_devices:\n",
    "            for node in input_graph_def.node:\n",
    "                node.device = \"\"\n",
    "        frozen_graph = tf.graph_util.convert_variables_to_constants(\n",
    "            session, input_graph_def, output_names, freeze_var_names)\n",
    "        return frozen_graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing Directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# healthy_img_path = os.listdir(\"fotos/Healthy/p01/\")[0]\n",
    "# healthy_img=image.load_img(\"fotos/Healthy/p01/\"+healthy_img_path)\n",
    "# healthy_img.size\n",
    "# plt.title(\"Healthy image\")\n",
    "# plt.imshow(healthy_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ng_img_path = os.listdir(\"fotos/ng/\")[0]\n",
    "# ng_img=image.load_img(\"fotos/ng/\"+ng_img_path)\n",
    "# ng_img.size\n",
    "# plt.title(\"NG image\")\n",
    "# plt.imshow(ng_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting Directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = \"./output/\"\n",
    "\n",
    "train_dir = os.path.join(base_dir, \"train\")\n",
    "train_good_dir = os.path.join(train_dir, \"Healthy\")\n",
    "train_defect_dir = os.path.join(train_dir, \"Sick\")\n",
    "\n",
    "# Test is used after training to test model\n",
    "test_dir = os.path.join(base_dir, \"test\")\n",
    "test_good_dir = os.path.join(test_dir, \"Healthy\")\n",
    "test_defect_dir = os.path.join(test_dir, \"Sick\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_good_train = len(os.listdir(train_good_dir))\n",
    "num_defect_train = len(os.listdir(train_defect_dir))\n",
    "\n",
    "num_good_test = len(os.listdir(test_good_dir))\n",
    "num_defect_test= len(os.listdir(test_defect_dir))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Characteristics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Training Good Images 3476\n",
      "Total Training Defect Images 825\n",
      "--\n",
      "Total Test Good Images 969\n",
      "Total Test Defect Images 224\n",
      "--\n",
      "Total Training Images 4301\n",
      "Total Testing Images 1193\n"
     ]
    }
   ],
   "source": [
    "print(\"Total Training Good Images\",num_good_train)\n",
    "print(\"Total Training Defect Images\",num_defect_train)\n",
    "print(\"--\")\n",
    "print(\"Total Test Good Images\", num_good_test)\n",
    "print(\"Total Test Defect Images\",num_defect_test)\n",
    "\n",
    "total_train = num_good_train+num_defect_train\n",
    "total_test = num_good_test+num_defect_test\n",
    "\n",
    "print(\"--\")\n",
    "print(\"Total Training Images\",total_train)\n",
    "print(\"Total Testing Images\",total_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Formatting Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_SHAPE  = (640, 480) # Size of images, assuming they are square.\n",
    "num_channels = 1 # Color channels of images (3 for color and 1 for grayscale).\n",
    "batch_size = 8 # Bigger batch size leads to a faster training, but can affect loss and accuracy metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4328 images belonging to 2 classes.\n",
      "Found 763 images belonging to 2 classes.\n",
      "Found 1401 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "image_gen_train = ImageDataGenerator(rescale = 1.0/255.0, \n",
    "                                    horizontal_flip=True, \n",
    "                                    rotation_range=20,\n",
    "                                    zoom_range=0.2,\n",
    "                                    width_shift_range=0.1,\n",
    "                                    height_shift_range=0.1,\n",
    "                                    fill_mode=\"nearest\",\n",
    "                                    validation_split=0.15)\n",
    "\n",
    "train_data_gen = image_gen_train.flow_from_directory(\n",
    "    batch_size = batch_size,\n",
    "    directory=train_dir,\n",
    "    shuffle=True,\n",
    "    target_size=IMG_SHAPE,\n",
    "    color_mode=\"grayscale\",\n",
    "    class_mode='binary',\n",
    "    subset=\"training\")\n",
    "\n",
    "validation_data_gen = image_gen_train.flow_from_directory(\n",
    "batch_size = batch_size,\n",
    "directory=train_dir,\n",
    "shuffle=True,\n",
    "target_size=IMG_SHAPE,\n",
    "color_mode=\"grayscale\",\n",
    "class_mode='binary',\n",
    "subset=\"validation\")\n",
    "\n",
    "image_gen_test = ImageDataGenerator(rescale=1.0/255.0)\n",
    "test_data_gen=image_gen_test.flow_from_directory(\n",
    "    batch_size=batch_size,\n",
    "    directory=test_dir,\n",
    "    shuffle=False,\n",
    "    target_size=IMG_SHAPE,\n",
    "    color_mode=\"grayscale\",\n",
    "    class_mode='binary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Healthy': 1, 'Sick': 0}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_gen.class_indices[\"Healthy\"] = 1\n",
    "train_data_gen.class_indices[\"Sick\"] = 0\n",
    "train_data_gen.class_indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\TE502801\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\ops\\init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv2D(32 , (3,3) , strides = 1 , padding = 'same' , activation = 'relu' , input_shape = (640, 480, 1)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPool2D((2,2) , strides = 2 , padding = 'same'))\n",
    "model.add(Conv2D(64 , (3,3) , strides = 1 , padding = 'same' , activation = 'relu'))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPool2D((2,2) , strides = 2 , padding = 'same'))\n",
    "model.add(Conv2D(64 , (3,3) , strides = 1 , padding = 'same' , activation = 'relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPool2D((2,2) , strides = 2 , padding = 'same'))\n",
    "model.add(Conv2D(128 , (3,3) , strides = 1 , padding = 'same' , activation = 'relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPool2D((2,2) , strides = 2 , padding = 'same'))\n",
    "model.add(Conv2D(256 , (3,3) , strides = 1 , padding = 'same' , activation = 'relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPool2D((2,2) , strides = 2 , padding = 'same'))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(units = 128 , activation = 'relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(units = 1 , activation = 'sigmoid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compiling model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\TE502801\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\ops\\nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\"acc\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 640, 480, 32)      320       \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 640, 480, 32)      128       \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 320, 240, 32)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 320, 240, 64)      18496     \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 320, 240, 64)      0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 320, 240, 64)      256       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 160, 120, 64)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 160, 120, 64)      36928     \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 160, 120, 64)      256       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 80, 60, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 80, 60, 128)       73856     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 80, 60, 128)       0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 80, 60, 128)       512       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 40, 30, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 40, 30, 256)       295168    \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 40, 30, 256)       0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 40, 30, 256)       1024      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 20, 15, 256)       0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 76800)             0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 128)               9830528   \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 10,257,601\n",
      "Trainable params: 10,256,513\n",
      "Non-trainable params: 1,088\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = 'termography_best.h5'\n",
    "\n",
    "# Model Checkpoint callback save the best model, not last.\n",
    "checkpoint = ModelCheckpoint(filepath=filepath, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "\n",
    "# Early Stopping callback will stop training if some monitor metric doesn't improve in n epochs (patience).\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5)\n",
    "\n",
    "callbacks = [checkpoint, early_stopping]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "536/537 [============================>.] - ETA: 7s - loss: 1.0475 - acc: 0.6670 \n",
      "Epoch 00001: val_loss improved from inf to 11.11491, saving model to termography_best.h5\n",
      "537/537 [==============================] - 4136s 8s/step - loss: 1.0468 - acc: 0.6664 - val_loss: 11.1149 - val_acc: 0.3591\n",
      "Epoch 2/30\n",
      "171/537 [========>.....................] - ETA: 53:31 - loss: 0.8615 - acc: 0.6703"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-21-85a915528bdf>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[0mbatch_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[0mverbose\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m     callbacks=callbacks)\n\u001b[0m\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[0mtotal_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mstart_time\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m    671\u001b[0m           \u001b[0muse_multiprocessing\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    672\u001b[0m           \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 673\u001b[1;33m           initial_epoch=initial_epoch)\n\u001b[0m\u001b[0;32m    674\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mtraining_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_eager_dataset_or_iterator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    675\u001b[0m       \u001b[1;31m# Make sure that y, sample_weights, validation_split are not passed.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[1;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[0;32m   1431\u001b[0m         \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1432\u001b[0m         \u001b[0minitial_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1433\u001b[1;33m         steps_name='steps_per_epoch')\n\u001b[0m\u001b[0;32m   1434\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1435\u001b[0m   def evaluate_generator(self,\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training_generator.py\u001b[0m in \u001b[0;36mmodel_iteration\u001b[1;34m(model, data, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch, mode, batch_size, steps_name, **kwargs)\u001b[0m\n\u001b[0;32m    262\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    263\u001b[0m       \u001b[0mis_deferred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_is_compiled\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 264\u001b[1;33m       \u001b[0mbatch_outs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mbatch_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    265\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    266\u001b[0m         \u001b[0mbatch_outs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[1;34m(self, x, y, sample_weight, class_weight, reset_metrics)\u001b[0m\n\u001b[0;32m   1173\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_update_sample_weight_modes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msample_weights\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weights\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1174\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1175\u001b[1;33m       \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=not-callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1176\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1177\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mreset_metrics\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\keras\\backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   3290\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3291\u001b[0m     fetched = self._callable_fn(*array_vals,\n\u001b[1;32m-> 3292\u001b[1;33m                                 run_metadata=self.run_metadata)\n\u001b[0m\u001b[0;32m   3293\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_fetch_callbacks\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfetched\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3294\u001b[0m     output_structure = nest.pack_sequence_as(\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1456\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[0;32m   1457\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1458\u001b[1;33m                                                run_metadata_ptr)\n\u001b[0m\u001b[0;32m   1459\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1460\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "n_epochs = 30\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "vgg_classifier = model.fit(train_data_gen,\n",
    "    steps_per_epoch=(total_train//batch_size),\n",
    "    validation_data=validation_data_gen,\n",
    "    epochs = n_epochs,\n",
    "    shuffle=True,\n",
    "    batch_size = batch_size,\n",
    "    verbose = 1,\n",
    "    callbacks=callbacks)\n",
    "\n",
    "total_time = time.time() - start_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"termography_last2.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting the training accuracy and loss\n",
    "# Training and validation accuracy:\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(vgg_classifier.history['acc'])\n",
    "plt.plot(vgg_classifier.history['val_acc'])\n",
    "plt.title('Model Accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train','test'],loc='upper left')\n",
    "plt.figure(figsize = (60,20))\n",
    "plt.show()\n",
    "\n",
    "# Summarize history for loss\n",
    "plt.plot(vgg_classifier.history['loss'],)\n",
    "plt.plot(vgg_classifier.history['val_loss'],)\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train','test'], loc = 'upper left')\n",
    "plt.figure(figsize = (20,10))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "print(os.getcwd())\n",
    "model = keras.models.load_model(\"termography_best.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = model.evaluate(test_data_gen, batch_size=batch_size)\n",
    "print(\"test_loss, test accuracy\", result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred = model.predict(test_data_gen,total_test//batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## extracting NG and G pred, then order and then \n",
    "ng_items = len(test_data_gen.classes) - np.count_nonzero(test_data_gen.classes)\n",
    "ng_ypred = np.copy(Y_pred[0:ng_items])\n",
    "g_items = np.count_nonzero(test_data_gen.classes)\n",
    "g_ypred = np.copy(Y_pred[ng_items:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "ng_analysis = [0,0,0,0,0,0,0,0,0,0]\n",
    "for i in range(ng_items):\n",
    "    if ng_ypred[i] <= .1:\n",
    "        ng_analysis[0] += 1\n",
    "    elif .1 < ng_ypred[i] <= .2:\n",
    "        ng_analysis[1] += 1\n",
    "    elif .2 < ng_ypred[i] <= .3:\n",
    "        ng_analysis[2] += 1\n",
    "    elif .3 < ng_ypred[i] <= .4:\n",
    "        ng_analysis[3] += 1\n",
    "    elif .4 < ng_ypred[i] <= .5:\n",
    "        ng_analysis[4] += 1\n",
    "    elif .5 < ng_ypred[i] <= .6:\n",
    "        ng_analysis[5] += 1\n",
    "    elif .6 < ng_ypred[i] <= .7:\n",
    "        ng_analysis[6] += 1\n",
    "    elif .7 < ng_ypred[i] <= .8:\n",
    "        ng_analysis[7] += 1\n",
    "    elif .8 < ng_ypred[i] <= .9:\n",
    "        ng_analysis[8] += 1\n",
    "    elif .9 < ng_ypred[i] <= 1:\n",
    "        ng_analysis[9] += 1\n",
    "    else: \n",
    "        0\n",
    "g_analysis = [0,0,0,0,0,0,0,0,0,0]\n",
    "for i in range(g_items):\n",
    "    if g_ypred[i] <= .1:\n",
    "        g_analysis[0] += 1\n",
    "    elif .1 < g_ypred[i] <= .2:\n",
    "        g_analysis[1] += 1\n",
    "    elif .2 < g_ypred[i] <= .3:\n",
    "        g_analysis[2] += 1\n",
    "    elif .3 < g_ypred[i] <= .4:\n",
    "        g_analysis[3] += 1\n",
    "    elif .4 < g_ypred[i] <= .5:\n",
    "        g_analysis[4] += 1\n",
    "    elif .5 < g_ypred[i] <= .6:\n",
    "        g_analysis[5] += 1\n",
    "    elif .6 < g_ypred[i] <= .7:\n",
    "        g_analysis[6] += 1\n",
    "    elif .7 < g_ypred[i] <= .8:\n",
    "        g_analysis[7] += 1\n",
    "    elif .8 < g_ypred[i] <= .9:\n",
    "        g_analysis[8] += 1\n",
    "    elif .9 < g_ypred[i] <= 1:\n",
    "        g_analysis[9] += 1\n",
    "    else: \n",
    "        0 \n",
    "        \n",
    "ng = pd.DataFrame(ng_analysis, columns = [''])\n",
    "ng = ng.set_index([pd.Index(['0-.1','.1-.2','.2-.3','.3-.4','.4-.5','.5-.6','.6-.7','.7-.8','.8-.9','.9-1'])])\n",
    "ng = ng.transpose()\n",
    "print('Prediction values distribution of Defected Parts')\n",
    "print(ng)\n",
    "\n",
    "g = pd.DataFrame(g_analysis, columns = [''])\n",
    "g = g.set_index([pd.Index(['0-.1','.1-.2','.2-.3','.3-.4','.4-.5','.5-.6','.6-.7','.7-.8','.8-.9','.9-1'])])\n",
    "g = g.transpose()\n",
    "print('Prediction values distribution of Good Parts')\n",
    "print(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statistics\n",
    "median_ng = statistics.median(ng_ypred)\n",
    "mean_ng = np.sum(ng_ypred)/len(ng_ypred)\n",
    "min_ng = np.max(ng_ypred)\n",
    "\n",
    "mean_g = np.sum(g_ypred)/len(g_ypred)\n",
    "median_g = statistics.median(g_ypred)\n",
    "min_g = np.min(g_ypred)\n",
    "\n",
    "print(\"Defected Images Median:\", median_ng)\n",
    "print(\"Defected Images Mean:\", mean_ng)\n",
    "print(\"Defected Images Maximum value:\", min_ng)\n",
    "\n",
    "print(\"Good Images Median:\",median_g)\n",
    "print(\"Good Images Mean:\",mean_g)\n",
    "print(\"Good Images Minimum value:\", min_g)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = .35"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confussion_preds = np.copy(Y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(test_data_gen.classes)):\n",
    "    if i < np.count_nonzero(test_data_gen.classes) : # iterate through pred of NG\n",
    "        if confussion_preds[i] <= threshold :\n",
    "            confussion_preds[i] = 0\n",
    "        else:\n",
    "            confussion_preds[i] = 1    \n",
    "    else:\n",
    "        if confussion_preds[i] >= threshold :\n",
    "            confussion_preds[i] = 1\n",
    "        else:\n",
    "            confussion_preds[i] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "cf_matrix = confusion_matrix(test_data_gen.classes, confussion_preds)\n",
    "\n",
    "print('Confusion Matrix')\n",
    "print(cf_matrix)\n",
    "print('Classification Report')\n",
    "target_names = ['Bad','Good']\n",
    "print(classification_report(test_data_gen.classes, confussion_preds, target_names=target_names))\n",
    "\n",
    "ax = sns.heatmap(cf_matrix, annot=True, cmap='Blues', fmt='g')\n",
    "\n",
    "ax.set_title('Seaborn Confusion Matrix with labels\\n\\n');\n",
    "ax.set_xlabel('\\nPredicted Values')\n",
    "ax.set_ylabel('Actual Values ');\n",
    "\n",
    "## Ticket labels - List must be in alphabetical order\n",
    "ax.xaxis.set_ticklabels(['Sick','Healthy'])\n",
    "ax.yaxis.set_ticklabels(['Sick','Healthy'])\n",
    "\n",
    "## Display the visualization of the Confusion Matrix.\n",
    "plt.show()\n",
    "\n",
    "plot = ax.get_figure()\n",
    "plot.savefig(\"confussion_matrix.jpg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NG IMAGES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = []\n",
    "folder_path = test_defect_dir\n",
    "images_list = os.listdir(folder_path)\n",
    "for img in images_list:\n",
    "    img_path = os.path.join(folder_path, img)\n",
    "    img = plt.imread(img_path)\n",
    "    images.append(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.patches import Rectangle\n",
    "color = 'none'\n",
    "true_negative = 0\n",
    "false_negative = 0\n",
    "\n",
    "to_show = 10\n",
    "nrows = 5\n",
    "ncols = 5\n",
    "\n",
    "i = 0\n",
    "for row in range(nrows):\n",
    "    row += 1\n",
    "    plt.figure(figsize=(20,10))\n",
    "    for col in range(ncols):\n",
    "        col += 1\n",
    "        # print(ng_ypred[i])\n",
    "        pred = f\"{ng_ypred[i, 0]:.4f}\"\n",
    "        \n",
    "        plt.subplot(1, ncols, col)\n",
    "        plt.text(IMG_SHAPE/2, IMG_SHAPE-5,\"Pred =\" + pred, color=\"orange\", fontdict={\"fontsize\":13,\"fontweight\":'bold',\"ha\":\"center\", \"va\":\"baseline\"})\n",
    "        \n",
    "        if ng_ypred[i] < threshold:\n",
    "            color = 'g'\n",
    "            true_negative = true_negative +1\n",
    "        else:\n",
    "            color = 'r'\n",
    "            false_negative = false_negative +1\n",
    "        plt.gca().add_patch(Rectangle((0,0),IMG_SHAPE,IMG_SHAPE,linewidth=5,edgecolor=color,facecolor='none'))\n",
    "\n",
    "        plt.imshow(images[i])\n",
    "        i += 1\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "print('True negative =' +' '+ str(true_negative))\n",
    "print('False negative =' + ' '+ str(false_negative))\n",
    "print ('Total evaluated parts =' + ' ' + str(true_negative+false_negative))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OK IMAGES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = []\n",
    "folder_path = test_good_dir\n",
    "images_list = os.listdir(folder_path)\n",
    "for img in images_list:\n",
    "    img_path = os.path.join(folder_path, img)\n",
    "    img = plt.imread(img_path)\n",
    "    images.append(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "color = 'none'\n",
    "true_positive = 0\n",
    "false_positive = 0\n",
    "\n",
    "print ('Evaluation on Good Parts')\n",
    "\n",
    "i = 0\n",
    "for row in range(nrows):\n",
    "    row += 1\n",
    "    plt.figure(figsize=(20,10))\n",
    "    for col in range(ncols):\n",
    "        col += 1\n",
    "        pred = f\"{g_ypred[i, 0]:.4f}\"\n",
    "        \n",
    "        plt.subplot(1, ncols, col)\n",
    "        plt.text(IMG_SHAPE/2, IMG_SHAPE-5,\"Pred =\" + pred, color=\"orange\", fontdict={\"fontsize\":13,\"fontweight\":'bold',\"ha\":\"center\", \"va\":\"baseline\"})\n",
    "        \n",
    "        if ng_ypred[i] < threshold:\n",
    "            color = 'g'\n",
    "            true_negative = true_negative +1\n",
    "        else:\n",
    "            color = 'r'\n",
    "            false_negative = false_negative +1\n",
    "        plt.gca().add_patch(Rectangle((0,0),IMG_SHAPE,IMG_SHAPE,linewidth=5,edgecolor=color,facecolor='none'))\n",
    "\n",
    "        plt.imshow(images[i])\n",
    "        i += 1\n",
    "\n",
    "    plt.show()\n",
    "    \n",
    "print('True positive =' +' '+ str(true_positive))\n",
    "print('False positive =' + ' '+ str(false_positive))\n",
    "print ('Total evaluated parts =' + ' ' + str(true_positive+false_positive))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## If saving last epoch in .pb from graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_node_names = [node.op.name for node in model.inputs]\n",
    "print('Input nodes names are:', str(input_node_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs_node_names=[node.op.name for node in model.outputs]\n",
    "print('Output nodes names are:', str(outputs_node_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frozen_graph = freeze_session(K.get_session(),output_names=[out.op.name for out in model.outputs])\n",
    "# Save to ./model/tf_model.pb\n",
    "tf.train.write_graph(frozen_graph, \"./\", \"ibc_gasket_training4.pb\", as_text=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Brief training report (.txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"training_report_fakra1011.txt\", \"w\") as f:\n",
    "    f.write(f\"Total number of epochs: {n_epochs} \\n\")\n",
    "    f.write(f\"Total time taken: {total_time/60} minutes \\n\")\n",
    "    f.write(\"\\nModel architecture: \\n\")\n",
    "    model.summary(print_fn=lambda x: f.write(x + \"\\n\"))\n",
    "    f.write(f\"\\nInput node name: {input_node_names[0]}\")\n",
    "    f.write(f\"\\nInput node name: {outputs_node_names[0]}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
